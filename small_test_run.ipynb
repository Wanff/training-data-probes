{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from utils import untuple\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### generate lots of model completions and store activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hooks import StatefulHook, InputHook, OutputHook\n",
    "\n",
    "class ModelWrapper():\n",
    "    \"\"\"\n",
    "    A wrapper for an autoregressive HF LM with hooking and activation storing functionality.\n",
    "    Supports GPT2 and Pythia models.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, device: str = \"cuda\"):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        self.model_type = \"pythia\" if \"pythia\" in model_name else \"gpt\"\n",
    "        \n",
    "        if self.model_type == \"pythia\":\n",
    "            self.num_layers = self.model.config.num_hidden_layers\n",
    "        elif self.model_type == \"gpt\":\n",
    "            self.num_layers = self.model.config.n_layer \n",
    "            \n",
    "        self.hooks = {}\n",
    "        self.save_ctx = {}\n",
    "\n",
    "    def query_model_tok_dist(self, prompt: str, K: int = 10) -> List[Tuple[float, str]]:\n",
    "        \"\"\"\n",
    "        Gets top 10 predictions and associated probabilities after last token in a prompt\n",
    "        \"\"\"\n",
    "        tokens = self.tokenizer.encode_plus(prompt, return_tensors = 'pt').to(self.device)\n",
    "        output = self.model(**tokens)\n",
    "        logits = output['logits']\n",
    "        \n",
    "        trg_tok_idx = tokens['input_ids'].shape[1] - 1\n",
    "        #gets probs after last tok in seq\n",
    "        probs = F.softmax(untuple(logits)[0][trg_tok_idx], dim=-1) #the [0] is to index out of the batch idx\n",
    "        probs = torch.reshape(probs, (-1,)).detach().cpu().numpy()\n",
    "\n",
    "        #assert probs add to 1\n",
    "        assert np.abs(np.sum(probs) - 1) <= 0.01, str(np.abs(np.sum(probs)-1)) \n",
    "\n",
    "        probs_ = []\n",
    "        for index, prob in enumerate(probs):\n",
    "            probs_.append((index, prob))\n",
    "\n",
    "        top_k = sorted(probs_, key = lambda x: x[1], reverse = True)[:K]\n",
    "        top_k = [(t[1].item(), self.tokenizer.decode(t[0])) for t in top_k]\n",
    "        \n",
    "        return top_k\n",
    "\n",
    "    def get_module(self, name):\n",
    "        \"\"\"\n",
    "        Finds the named module within the given model.\n",
    "        \"\"\"\n",
    "        for n, m in self.model.named_modules():\n",
    "            if n == name:\n",
    "                return m\n",
    "        raise LookupError(name)\n",
    "    \n",
    "    def remove_all_hooks(self):\n",
    "        for name, hook in self.hooks.items():\n",
    "            hook.remove()\n",
    "        \n",
    "        self.hooks = {}\n",
    "    \n",
    "    def register_layer_hooks(self):\n",
    "        for i in range(self.num_layers):\n",
    "            if self.model_type == \"pythia\":\n",
    "                layer_name = f\"gpt_neox.layers.{i}\"\n",
    "            elif self.model_type == \"gpt\":\n",
    "                layer_name = f\"transformer.h.{i}\"\n",
    "            self.register_stateful_hook(layer_name, OutputHook(layer_name))\n",
    "                \n",
    "    def register_stateful_hook(self, module_name:str, stateful_hook:StatefulHook):\n",
    "        module = self.get_module(module_name)\n",
    "        \n",
    "        self.hooks[stateful_hook.name] = module.register_forward_hook(stateful_hook) #saves the handle to the hooks dict\n",
    "        self.save_ctx[stateful_hook.name] = stateful_hook #saves the activations to the save_ctx dict\n",
    "\n",
    "    def register_dir_add_hook(self, module_name: str, intervention_idxs: List[int], dir : torch.Tensor, alpha: float = 1.0):\n",
    "        def hook(module, input, output):\n",
    "            for idx in intervention_idxs:\n",
    "                output[0][:,idx,:] += dir * alpha\n",
    "            return output\n",
    "        \n",
    "        module = self.get_module(module_name)\n",
    "        self.hooks[module_name] =module.register_forward_hook(hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-160m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_data = load_dataset('EleutherAI/pythia-memorized-evals', split='duped.160m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "\n",
    "model.to(device)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "i = 0\n",
    "input_ids = torch.tensor([mem_data[i]['tokens'][:32]])\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "out= model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask = torch.ones_like(input_ids).to(device),\n",
    "    max_new_tokens=32,\n",
    "    top_p=1.0,\n",
    "    output_hidden_states=True,\n",
    "    return_dict_in_generate = True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resolving data files: 100%|██████████| 1650/1650 [00:01<00:00, 914.09it/s]\n"
     ]
    }
   ],
   "source": [
    "pile = datasets.load_dataset('EleutherAI/the_pile_deduplicated', split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It is done, and submitted. You can play “Survival of the Tastiest” on Android, and on the web. Playing on the web works, but you have to simulate multi-touch for table moving and that can be a bit confusing.\\n\\nThere’s a lot I’d like to talk about. I’ll go through every topic, insted of making the typical what went right/wrong list.\\n\\nConcept\\n\\nWorking over the theme was probably one of the hardest tasks I had to face.\\n\\nOriginally, I had an idea of what kind of game I wanted to develop, gameplay wise – something with lots of enemies/actors, simple graphics, maybe set in space, controlled from a top-down view. I was confident I could fit any theme around it.\\n\\nIn the end, the problem with a theme like “Evolution” in a game is that evolution is unassisted. It happens through several seemingly random mutations over time, with the most apt permutation surviving. This genetic car simulator is, in my opinion, a great example of actual evolution of a species facing a challenge. But is it a game?\\n\\nIn a game, you need to control something to reach an objective. That control goes against what evolution is supposed to be like. If you allow the user to pick how to evolve something, it’s not evolution anymore – it’s the equivalent of intelligent design, the fable invented by creationists to combat the very idea of evolution. Being agnostic and a Pastafarian, that’s not something that rubbed me the right way.\\n\\nHence, my biggest dillema when deciding what to create was not with what I wanted to create, but with what I did not. I didn’t want to create an “intelligent design” simulator and wrongly call it evolution.\\n\\nThis is a problem, of course, every other contestant also had to face. And judging by the entries submitted, not many managed to work around it. I’d say the only real solution was through the use of artificial selection, somehow. So far, I haven’t seen any entry using this at its core gameplay.\\n\\nAlas, this is just a fun competition and after a while I decided not to be as strict with the game idea, and allowed myself to pick whatever I thought would work out.\\n\\nMy initial idea was to create something where humanity tried to evolve to a next level but had some kind of foe trying to stop them from doing so. I kind of had this image of human souls flying in space towards a monolith or a space baby (all based in 2001: A Space Odyssey of course) but I couldn’t think of compelling (read: serious) mechanics for that.\\n\\nBorgs were my next inspiration, as their whole hypothesis fit pretty well into the evolution theme. But how to make it work? Are you the borg, or fighting the Borg?\\n\\nThe third and final idea came to me through my girlfriend, who somehow gave me the idea of making something about the evolution of Pasta. The more I thought about it the more it sounded like it would work, so I decided to go with it.\\n\\nConversations with my inspiring co-worker Roushey (who also created the “Mechanical Underdogs” signature logo for my intros) further matured the concept, as it involved into the idea of having individual pieces of pasta flying around and trying to evolve until they became all-powerful. A secondary idea here was that the game would work to explain how the Flying Spaghetti Monster came to exist – by evolving from a normal dinner table.\\n\\nSo the idea evolved more or less into this: you are sitting a table. You have your own plate, with is your “base”. There are 5 other guests at the table, each with their own plate.\\n\\nYour plate can spawn little pieces of pasta. You do so by “ordering” them through a menu. Some pastas are better than others; some are faster, some are stronger. They have varying costs, which are debited from your credits (you start with a number of credits).\\n\\nOnce spawned, your pastas start flying around. Their instinct is to fly to other plates, in order to conquer them (the objective of the game is having your pasta conquer all the plates on the table). But they are really autonomous, so after being spawned, you have no control over your pasta (think DotA or LoL creeps).\\n\\nYour pasta doesn’t like other people’s pasta, so if they meet, they shoot sauce at each other until one dies. You get credits for other pastas your own pasta kill.\\n\\nOnce a pasta is in the vicinity of a plate, it starts conquering it for its team. It takes around 10 seconds for a plate to be conquered; less if more pasta from the same team are around. If pasta from other team are around, though, they get locked down in their attempt, unable to conquer the plate, until one of them die (think Battlefield’s standard “Conquest” mode).\\n\\nYou get points every second for every plate you own.\\n\\nOver time, the concept also evolved to use an Italian bistro as its main scenario.\\n\\nCarlos, Carlos’ Bistro’s founder and owner\\n\\nSetup\\n\\nNo major changes were made from my work setup. I used FDT and Starling creating an Adobe AIR (ActionScript) project, all tools or frameworks I already had some knowledge with.\\n\\nOne big change for me was that I livestreamed my work through a twitch.tv account. This was a new thing for me. As recommended by Roushey, I used a program called XSplit and I got to say, it is pretty amazing. It made the livestream pretty effortless and the features are awesome, even for the free version. It was great to have some of my friends watch me, and then interact with them and random people through chat. It was also good knowing that I was also recording a local version of the files, so I could make a timelapse video later.\\n\\nKnowing the video was being recorded also made me a lot more self-conscious about my computer use, as if someone was watching over my shoulder. It made me realize that sometimes I spend too much time in seemingly inane tasks (I ended up wasting the longest time just to get some text alignment the way I wanted – it’ll probably drive someone crazy if they watch it) and that I do way too many typos where writing code. I pretty much spend half of the time writing a line and the other half fixing the crazy characters in it.\\n\\nMy own stream was probably boring to watch since I was coding for the most time. But livestreaming is one of the cool things to do as a spectator too. It was great seeing other people working – I had a few tabs opened on my second monitor all the time. It’s actually a bit sad, because if I could, I could have spent the whole weekend just watching other people working! But I had to do my own work, so I’d only do it once in a while, when resting for a bit.\\n\\nDesign\\n\\nAlthough I wanted some simple, low-fi, high-contrast kind of design, I ended up going with somewhat realistic (vector) art. I think it worked very well, fitting the mood of the game, but I also went overboard.\\n\\nFor example: to know the state of a plate (who owns it, who’s conquering it and how much time they have left before conquering it, which pasta units are in the queue, etc), you have to look at the plate’s bill.\\n\\nThe problem I realized when doing some tests is that people never look at the bill! They think it’s some kind of prop, so they never actually read its details.\\n\\nPlus, if you’re zoomed out too much, you can’t actually read it, so it’s hard to know what’s going on with the game until you zoom in to the area of a specific plate.\\n\\nOne other solution that didn’t turn out to be as perfect as I thought was how to indicate who a plate base belongs to. In the game, that’s indicated by the plate’s decoration – its color denotes the team owner. But it’s something that fits so well into the design that people never realized it, until they were told about it.\\n\\nIn the end, the idea of going with a full physical metaphor is one that should be done with care. Things that are very important risk becoming background noise, unless the player knows its importance.\\n\\nOriginally, I wanted to avoid any kind of heads-up display in my game. In the end, I ended up adding it at the bottom to indicate your credits and bases owned, as well as the hideous out-of-place-and-still-not-obvious “Call Waiter” button. But in hindsight, I should have gone with a simple HUD from the start, especially one that indicated each team’s colors and general state of the game without the need for zooming in and out.\\n\\nDevelopment\\n\\nDevelopment went fast. But not fast enough.\\n\\nEven though I worked around 32+ hours for this Ludum Dare, the biggest problem I had to face in the end was overscoping. I had too much planned, and couldn’t get it all done.\\n\\nContent-wise, I had several kinds of pasta planned (Wikipedia is just amazing in that regard), split into several different groups, from small Pastina to huge Pasta al forno. But because of time constraints, I ended up scratching most of them, and ended up with 5 different types of very small pasta – barely something to start when talking about the evolution of Pasta.\\n\\nPastas used in the game. Unfortunately, the macs where never used\\n\\nWhich is one of the saddest things about the project, really. It had the framework and the features to allow an endless number of elements in there, but I just didn’t have time to draw the rest of the assets needed (something I loved to do, by the way).\\n\\nOther non-obvious features had to be dropped, too. For example, when ordering some pasta, you were supposed to select what kind of sauce you’d like with your pasta, each with different attributes. Bolognese, for example, is very strong, but inaccurate; Pesto is very accurate and has great range, but it’s weaker; and my favorite, Vodka, would triggers 10% loss of speed on the pasta hit by it.\\n\\nThe code for that is mostly in there. But in the end, I didn’t have time to implement the sauce selection interface; all pasta ended up using bolognese sauce.\\n\\nTo-do list: lots of things were not done\\n\\nActual programming also took a toll in the development time. Having been programming for a while, I like to believe I got to a point where I know how to make things right, but at the expense of forgetting how to do things wrong in a seemingly good way. What I mean is that I had to take a lot of shortcuts in my code to save time (e.g. a lot of singletons references for cross-communication rather than events or observers, all-encompassing check loops, not fast enough) that left a very sour taste in my mouth. While I know I used to do those a few years ago and survive, I almost cannot accept the state my code is in right now.\\n\\nAt the same time, I do know it was the right thing to do given the timeframe.\\n\\nOne small thing that had some impact was using a somewhat new platform for me. That’s Starling, the accelerated graphics framework I used in Flash. I had tested it before and I knew how to use it well – the API is very similar to Flash itself. However, there were some small details that had some impact during development, making me feel somewhat uneasy the whole time I was writing the game. It was, again, the right thing to do, but I should have used Starling more deeply before (which is the conundrum: I used it for Ludum Dare just so I could learn more about it).\\n\\nArgument and user experience\\n\\nOne final aspect of the game that I learned is that making the game obvious for your players goes a long way into making it fun. If you have to spend the longest time explaining things, your game is doing something wrong.\\n\\nAnd that’s exactly the problem Survival of the Tastiest ultimately faced. It’s very hard for people to understand what’s going on with the game, why, and how. I did have some introductory text at the beginning, but that was a last-minute thing. More importantly, I should have had a better interface or simplified the whole concept so it would be easier for people to understand.\\n\\nThat doesn’t mean the game itself should be simple. It just means that the experience and interface should be approachable and understandable.\\n\\nConclusion\\n\\nI’m extremely happy with what I’ve done and, especially given that this was my first Ludum Dare. However, I feel like I’ve learned a lot of what not to do.\\n\\nThe biggest problem is overscoping. Like Eric Decker said, the biggest lesson we can learn with this is probably with scoping – deciding what to do beforehand in a way you can complete it without having to rush and do something half-assed.\\n\\nI’m sure I will do more Ludum Dares in the future. But if there are any lessons I can take of it, they are to make it simple, to use frameworks and platforms you already have some absolute experience with (otherwise you’ll spend too much time trying to solve easy questions), and to scope for a game that you can complete in one day only (that way, you can actually take two days and make it cool).\\n\\nThis entry was posted\\non Monday, August 27th, 2012 at 10:54 am and is filed under LD #24.\\nYou can follow any responses to this entry through the RSS 2.0 feed.\\nYou can skip to the end and leave a response. Pinging is currently not allowed.\\n\\n3 Responses to ““Survival of the Tastiest” Post-mortem”\\n\\ndarn it , knowing that I missed your livestream makes me a sad panda ;( but more to the point, the game is … well for a startup its original to say the least ;D it has some really neat ideas and more importantly its designed arround touch screens whitch by the looks of the submission is something rare ;o or that could be just me and my short memory -_-! awesum game, love et <3',\n",
       " 'Topic: reinvent midnight madness\\n\\nAmazon announced a new service at the AWS re:Invent Midnight Madness event. Amazon Sumerian is a solution that aims to make it easier for developers to build virtual reality, augmented reality, and 3D applications. It features a user friendly editor, which can be used to drag and drop 3D objects and characters into scenes. Amazon … continue reading',\n",
       " 'About Grand Slam Fishing Charters\\n\\nAs a family owned business we know how important it is that your trip becomes the best memory of your vacation, we are proud of our islands, our waters and our crew and we are desperate show you the best possible time during your stay. We can not guarantee fish every time but we can guarantee you a great time! The biggest perk of our job is seeing so many of our customers become close friends”\\n\\nA Great Way To Make New Friends!\\n\\nOur dockside parties are a great way to make new friends! Everyone is welcome!\\n\\nAndrea runs the whole operation, from discussing your initial needs by phone or email through to ensuring you have sufficient potato chips. Andrea has worked as concierge for many International resorts and fully understands the high expectations of international visitors.\\n\\n“Life’s A Game But Fishing Is Serious!”\\n\\nUnlike many tour operators, our crew are highly valued and have been with us since day 1. Each have their own personalities and sense of humour and understand the importance of making your day perfect, for us the saying is true, “Lifes a game but fishing is serious!”\\n\\nTRIP ADVISOR\\n\\nPlan Your Trip!\\n\\nAJ and Earl were excellent. My son and I did a half day deep sea trip and though the fish weren’t too cooperative, they did everything to try to get something to bite. Very knowledgeable about the waters and my son was able to land a nice barracuda. The next day my wife, daughter, son […]\\n\\nWhen we arrived the crew made us feel right at home. They made us feel comfortable and answered all questions. The crew worked hard all day to put us on fish. We were successful in landing a nice size Wahoo even though the weather did not cooperate the entire day was enjoyable. I highly recommend […]',\n",
       " 'Working Women, Special Provision and the Debate on Equality\\n\\nThere has been considerable coverage in the media recently about the possibility of offering women in employment paid leave from work during their menstrual period. This has generated a broad range of responses relating to long-standing discussions about ‘equality’ and ‘difference’: is women’s equality best achieved by treating them the same as men or by making provisions that recognise their differences in terms of physiological constitution and biological functions?\\n\\nIf the UK introduces such an initiative, it would not be the first country in the contemporary world to do so. Many countries in Asia already make the provision and Russia debated introducing a law in 2013. The policy also has a significant historical precedent. A whole chapter of my book Women Workers in the Soviet Interwar Economy: From ‘Protection’ to ‘Equality’ (Macmillan, 1999), based on extensive research conducted for my PhD, is devoted to ‘Provision for “Menstrual Leave”’.\\n\\nIn the 1920s, scientific researchers and labour hygiene specialists in the Soviet Union conducted extensive investigations into the impact of menstruation on women’s capacity to work in manual and industrial jobs requiring a significant degree of physical labour. Their recommendations led to two decrees being issued that targeted specific categories of women workers:\\n\\nDecree ‘On the release from work during menstruation of machinists and iron press workers working on cutting machines without mechanised gears in the garment industry’, 11 January 1922\\n\\nDecree ‘On the working conditions of women tractor and lorry drivers’, 9 May 1931\\n\\nThese decrees arose from research that suggested, amongst other things, that inadequate seating at machines and on tractors resulted in congestion and tension in the abdomen that was exacerbated during menstruation. In practice, the decrees did not provide for regular absence from work. Women seeking to benefit from the provision had to provide a doctor’s note, similar to the usual requirements for sick leave.\\n\\nThe official research into the impact of menstruation on women’s capacity to work and the application of the decrees in practice raised a number of issues on both sides of the argument. I offer only a summary of the contemporary research findings and observer commentary here:\\n\\nFor the provision:\\n• employers have a responsibility to protect the health of their workers and unhealthy, poor and inadequate working environments can have a detrimental impact on women’s reproductive health\\n• women’s labour productivity and output would rise as a result\\n• it is essential to protect the professionalism of certain categories of workers: the debates here centred on performance artists and female theatrical employees engaged in highly physical and intensely emotional work\\n• heavy physical labour and strenuous exercise can lead to disruptions of the menstrual cycle\\n• women’s physical and intellectual capacities are reduced during menstruation; women lose muscular strength and powers of concentration\\n• women’s biological constitution and reproductive functions require specific recognition in law\\n\\nAgainst the provision:\\n• employers are less likely to appoint women if they are guaranteed paid time off work during menstruation\\n• (often from male workers, who viewed the employment of women as competition) women should not be employed in jobs for which they lack the physical strength and mental capacity\\n• if necessary, women could be transferred to different tasks involving easier work during menstruation\\n• the provision would be open to uneven application and abuse\\n• women cannot expect to be considered equal with men if they are given special treatment in the law\\n\\nIt is worth noting also that the various research projects often revealed that the vast majority of women reported no regular problems or abnormalities with menstruation, and that men commonly reported higher levels of sickness than their female colleagues. Many of the problems experienced by women in the workplace could be mitigated by the introduction of improvements to their physical working conditions (not sitting down or standing up in the same position for long periods of time) or by the simple introduction of very short breaks that would allow women to walk around and get some exercise.\\n\\nDebates in the UK, on the TV and in the press, are unlikely to reach a consensus on this issue. What do you think?',\n",
       " 'Jeanette Sawyer Cohen, PhD, clinical assistant professor of psychology in pediatrics at Weill Cornell Medical College in New York City\\n\\nPediatric Psychologist\\n\\nHow to Teach Independence?\\n\\nHow can I teach my toddler to do things independently?\\n\\nYou’ve probably become more patient since you started this whole parenthood thing. And you’re going to have to practice patience even more as your toddler learns to become more independent.\\n\\nFor example, she tells you she can’t finish the puzzle she’s doing. Instead of jumping right in and telling her which piece goes where, you’re going to have to tell her you’ll help a little. Go ahead and help, but let her do a lot of it herself, and make sure she’s the one to finish the job. That will give her a sense of accomplishment and the confidence to try again next time.\\n\\nRemember that children each progress at their own rate. It’s not always fast — and there will be setbacks along the way. But the more you can allow them to do on their own without stepping in, the more they’ll be likely to try for themselves again and again.',\n",
       " '{\\n  \"fpsLimit\": 60,\\n  \"preset\": \"basic\",\\n  \"background\": {\\n    \"color\": \"#0d47a1\",\\n    \"image\": \"\",\\n    \"position\": \"50% 50%\",\\n    \"repeat\": \"no-repeat\",\\n    \"size\": \"cover\"\\n  }\\n}',\n",
       " 'Running\\n\\nStat\\n\\nDinner with people is always better than eating alone, especially when the food is good. Good food tastes even better when enjoyed with people. Tonight Amy came over to try my second attempt at the Brussels Sprouts Veggie Soup to which I have made some changes (see recipe below in previous post) for a better result, I believe.\\n\\nWe were at the store earlier and saw some nice looking haricot verts and heirloom tomatoes, so we decide to assemble a simple salad from those. Of course while I’m at the market, I can’t not get some five peppercorn salami. Our simple dinner of soup, salami, bread, cheese, salad, and wine was on the table in 15 minutes.',\n",
       " 'jOOQ on The ORM Foundation?\\n\\nI am the developer of jOOQ, a Java database abstraction framework. I was wondering whether jOOQ might be an interesting tool for discussion on your website, even if it is not exactly an ORM in the classic meaning (as in mapping objects to the relational world > ORM). Instead, jOOQ uses a reverse engineering paradigm (as in mapping relational entities to objects > \"ROM\").\\n\\nRe: jOOQ on The ORM Foundation?\\n\\nObject Role Modeling (the original ORM) is not the same thing as Object/Relational Mapping.\\n\\nObject/Relational Mapping is still kind-of relevant and interesting to us, since Object Role Modeling is used to design databases (which then will require programmatic access). But there are probably better places to discuss it :]\\n\\nYour query DSL looks rather like some of the DSLs available for Ruby, such as through the Sequel gem, or Arel. Interesting to see how well that can work with a statically-types language like Java. Maybe you or I should make a generator for ActiveFacts which generates your DSL from CQL queries?\\n\\nRe: jOOQ on The ORM Foundation?\\n\\nSorry for my late reply. Apparently I had not really understood the ideas behind your foundation when I wrote my original post. I understand now, that you are concerned with broader concepts than the \"common ORM\". I actually came across your group because of your linking to ORM Lite (where ORM does stand for Object/Relational Mapping, correct me if I\\'m wrong).\\n\\nYes, I have seen some examples for Ruby\\'s Sequel. I personally find statically-typed languages much better for DSL\\'s as the syntax can be formally defined and checked by a compiler - with the limitations an OO language imposes, of course.\\n\\nSo if I understand this correctly now, \"Object Role Modeling\" and CQL are actually a more general way of expressing what SQL calls DDL. Since you can already transform CQL into SQL DDL statements (CREATE TABLE...), and jOOQ can reverse-engineer database schemata into jOOQ generated source code, I don\\'t think there would be need for an additional generator.\\n\\nDoes CQL also specify means of querying the data described by the Object Role Model? The examples I found here only seem to describe what SQL calls \"constraints\" (although with a much broader functionality-range than SQL):\\n\\nRe: jOOQ on The ORM Foundation?\\n\\n\"common ORM\". I actually came across your group because of your linking to ORM Lite (where ORM does stand for Object/Relational Mapping\\n\\nObject Role Modeling was named before Object Relational Mapping, but the latter is now the more common meaning, as you point out. But ORM Lite is actually so-named by Bryan because it is an implementation of Object Role Modeling, not because it is also an O/RM. Bryan was a student of Terry\\'s at Neumont, where he learnt ORM.\\n\\nRegarding DSLs, I think internal DSLs only work well in very simple cases. I prefer external DSLs for anything complex, and that\\'s where CQL came from. Even the extremely flexible syntax of Ruby wasn\\'t up to the task.\\n\\nlukas.eder:\\n\\nI don\\'t think there would be need for an additional generator\\n\\nThe problem is that a huge amount of meaning is lost in the mapping to SQL. SQL is practically (though not theoretically) limited to representing physical models. These are almost always very different from the conceptual model, as many relationships have been condensed (absorbed) into attribute/column relationships, so the semantics of the original relationship are lost. In the process, nullable columns are usually introduced, which adds further to the confusion, as such things cannot easily be correctly constrained (uniqueness, etc) in SQL. So by reverse engineering from the relational form, you\\'re losing most of the benefit of building a conceptual model from the start\\n\\nThis may be hard to see for someone used to O-O modeling, and who\\'s authored an O/RM tool. The problem is that O-O suffers from many of the same problems of loss of semantics. The apparently clear notion of \"attribute\" breaks down when you look at it closely. O-O, although ostensibly behaviour-oriented, introduces attributes to store state, and this attribute orientation is the source of the problem in both cases. Fact-oriented model does not use attributes. Although it may seem obvious that, for example, my surname is an attribute of myself, if the system being modeled accrues the requirement to model families, suddenly surname becomes an attribute of family, and family becomes my attribute. This kind of instability is responsible for much of the rework that\\'s required in evolving legacy systems, as well as many of the mistakes made when they were first modeled. If you want a further example of this loss of semantics, look at my Insurance example, and ask yourself why the VehicleIncident table has a DrivingBloodTestResult column. In fact, if VehicleIncident wasn\\'t explicitly mapped separately, its fields would be in the Claim table.\\n\\nWhat\\'s needed is not just yet another O/RM tool (which are tuppence a dozen anyhow - I personally have written three) but a tool which supports database programming using only the conceptual model, never exposing the physical model. Surprisingly, I can\\'t think of a single tool which has done a good job of this, but it\\'s where I\\'m heading with the ActiveFacts API. It\\'s another O/RM, but using a purely conceptual object model that preserves the domain semantics, not a typical O-O one.\\n\\nlukas.eder:\\n\\nDoes CQL also specify means of querying the data described by the Object Role Model\\n\\nYes, though the published implementation doesn\\'t quite handle the full query syntax (aggregate functions are still missing), nor does it yet translate them to SQL. Some examples are given towards the end of the video presentation on the CQL Introduction page.\\n\\nRe: jOOQ on The ORM Foundation?\\n\\nRegarding DSLs, I think internal DSLs only work well in very simple cases. I prefer external DSLs for anything complex, and that\\'s where CQL came from. Even the extremely flexible syntax of Ruby wasn\\'t up to the task.\\n\\nAbsolutely. The optimal way to implement SQL in Java would be by extending the Java language itself, such that SQL would be compiled natively by the java compiler, similar to Linq2SQL in C#, or PL/SQL in Oracle databases. So for the complexity of CQL, CQL is certainly the right solution.\\n\\nClifford Heath:\\n\\nThe problem is that a huge amount of meaning is lost in the mapping to SQL. SQL is practically (though not theoretically) limited to representing physical models.\\n\\nYou are right. I guess though, that in everyday work, this limitation is not really a problem. Personally, I think if your business rules become so complex that you cannot map them to a relational model easily anymore, then maybe your business rules could be simplified before changing/extending technologies. But that depends on the business, of course. I guess with insurance companies\\' businesses, I\\'d be pretty lost, personally ;-)\\n\\nIn any case, I don\\'t see jOOQ as a means to solve modelling issues, or the O/R impedance mismatch (which is even bigger when it comes to mapping your understanding of ORM, with CQL). jOOQ should simply make using the full power of SQL in Java as simple as possible. In that way, jOOQ is not really an ORM because it does not map from objects to the relational world, or try to solve any other high-level abstraction issues. It\\'s really a low-level tool to make a developer\\'s life a lot easier, seeing that unfortunately, JPA CriteriaQuery didn\\'t meet the community\\'s expectations.\\n\\nClifford Heath:\\n\\nWhat\\'s needed is not just yet another O/RM tool (which are tuppence a dozen anyhow - I personally have written three) but a tool which supports database programming using only the conceptual model, never exposing the physical model. Surprisingly, I can\\'t think of a single tool which has done a good job of this, but it\\'s where I\\'m heading with the ActiveFacts API. It\\'s another O/RM, but using a purely conceptual object model that preserves the domain semantics, not a typical O-O one.\\n\\nI think you\\'re on the right track with this. I hope for you, that this will soon show nice results with a practical implementation. I\\'m curious to see how you\\'ll tackle performance issues, too, with all the abstraction. Among all attempts to overcome the old and proven relational models (XML databases, NoSQL databases), this one seems the most promising and focused to me!',\n",
       " 'I\\'ve learned the nitrogen vacancies used in Memristors are for \"switching\", between excited states and inhibited states, akin to our neurons and SYNAPSES abilities to generate EPSPs and IPSPs, this is the entire point to Memristors and DARPAs SyNAPSE program, emulating Neurons..\\n\\nSo in the memristor, NVs (which are truly Ancillas),\\nReturn to \"resting states\", just like Neurons do, hence Inhibitory states versus excited states, when a neuron reaches an action potential and fires..\\n\\nSo the ancillas use prepared/ known states, and are the equivalent of the ancillas ground state, which is equal to a neurons resting potential...\\n\\nSo by weakly measuring certain aspects of living neurons, it is possible to superbroadcast/ teleport the wavefunction non-classically to the memristors vacancies, correlating each memristor with its neuron statistical ensemble counterpart, sharing the quantum state of the resting potential.\\nthe ground state of the ancilla.\\n\\nThe type of measurement determines which property is shown. However the single and double-slit experiment and other experiments show that some effects of wave and particle can be measured in one measurement.\\n\\nHence Mach-Zehnder interferometry, which also involves ANCILLAS\\n\\nQuote:\\n\\nWhen for example measuring a photon using a Mach-Zehnder interferometer, the photon acts as a wave if the second beam-splitter is inserted, but as a particle if this beam-splitter is omitted. The decision of whether or not to insert this beam-splitter can be made after the photon has entered the interferometer, as in Wheeler’s famous delayed-choice thought experiment. In recent quantum versions of this experiment, this decision is controlled by a quantum ancilla, while the beam splitter is itself still a classical object.\\n\\nand the no-cloning theorem is about pure states..\\nBut an ensemble of particles in a neuron would make it a mixed state..\\n\\nThe no-cloning theorem is normally stated and proven for pure states; the no-broadcast theorem generalizes this result to mixed states.\\n\\nAnd thats why PHASE works for quantum metrology and its ability to harness non classical states\\n\\nApparently, worrying about measuring both position and momentum works differently for particles than it does waves.\\n\\nIt may actually be possible using phase.\\n\\nQuote:\\n\\nNiels Bohr apparently conceived of the principle of complementarity during a skiing vacation in Norway in February and March 1927, during which he received a letter from Werner Heisenberg regarding the latter\\'s newly discovered (and not yet published) uncertainty principle. Upon returning from his vacation, by which time Heisenberg had already submitted his paper on the uncertainty principle for publication, he convinced Heisenberg that the uncertainty principle was a manifestation of the deeper concept of complementarity.[6] Heisenberg duly appended a note to this effect to his paper on the uncertainty principle, before its publication, stating:\\n\\nQuote:\\n\\nBohr has brought to my attention [that] the uncertainty in our observation does not arise exclusively from the occurrence of discontinuities, but is tied directly to the demand that we ascribe equal validity to the quite different experiments which show up in the [particulate] theory on one hand, and in the wave theory on the other hand.\\n\\nAnd \"quadratures\" is about position and momentum..\\nWhich are apparently always orthogonal to each other.\\n\\nThere is obviously something to all of this.\\nCounterfactual Communication was recently used to transmit information without sending any PARTICLES.\\nthe information was sent in the phase.. of a wavefunction?\\n\\nand it used MachZenhder Interferometry..\\nwhich is part of Quantum Metrology and its ability to harness non-classical states..\\n\\nand all of this can teleport non-classical light..\\n\\nand it all uses ANCILLAS... which store VALUES, and WAVEFUNCTIONS.. because they are Qubits/ Nitrogen vacancies..\\n\\nand are used in WEAK MEASUREMENT... which was used to measure a wavefunction.. something most would argue is impossible.. because of the uncertainty principle..\\n\\nQuote:\\n\\nAn interpretation of quantum mechanics can be said to involve the use of counterfactual definiteness if it includes in the statistical population of measurement results, any measurements that are counterfactual because they are excluded by the quantum mechanical impossibility of simultaneous measurement of conjugate pairs of properties.\\n\\nFor example, the Heisenberg uncertainty principle states that one cannot simultaneously know, with arbitrarily high precision, both the position and momentum of a particle\\n\\nQuote:\\n\\nThe word \"counterfactual\" does not mean \"characterized by being opposed to fact.\" Instead, it characterizes values that could have been measured but, for one reason or another, were not\\n\\nand its the Ancillas that store values.. and may or may not be part of the measurement apparatus... / interferometer..\\n\\nIn 2015, Counterfactual Quantum Computation was demonstrated in the experimental context of \"spins of a negatively charged Nitrogen-vacancy color center in a diamond\".[5] Previously suspected limits of efficiency were exceeded, achieving counterfactual computational efficiency of 85% with the higher efficiency foreseen in principle\\n\\nQuote:\\n\\nThe quantum computer may be physically implemented in arbitrary ways but the common apparatus considered to date features a Mach–Zehnder interferometer. The quantum computer is set in a superposition of \"not running\" and \"running\" states by means such as the Quantum Zeno Effect. Those state histories are quantum interfered. After many repetitions of very rapid projective measurements, the \"not running\" state evolves to a final value imprinted into the properties of the quantum computer. Measuring that value allows for learning the result of some types of computations such as Grover\\'s algorithm even though the result was derived from the non-running state of the quantum computer.\\n\\nNV CENTERS can also be used asQUANTUM SPIN PROBES, QUBITS & AS, ANCILLAS\\n\\nin devices such as\\nBIOMEMs scanners\\nQUANTUM REPEATERS\\nPHOTONIC NETWORKING\\n\\nand..\\n\\nMEMRISTORS.. where the vacancies are used for switching between inhibited and excited states, thus simulating NEURONS\\n\\nMEMRISTORS utilize wavefunctions.\\n\\nWavefunctions can be weakly measured by ANCILLAS\\n\\nANCILLAS hold \"values\" ie : wavefunctions\\n\\nand have GROUND STATES\\n\\nwhich measured particles are \"cooled\" into for measurement techniques. a literal form of \"photon counting\"..\\n\\n\"This de-excitation is called ‘fluorescence’, and it is characterized by a\\nlifetime of a few nanoseconds of the lowest vibrational level of the first excited state.\\nDe-excitation from the excited singlet state to the ground state also occurs by other mechanisms, such as non-radiant thermal decay or ‘phosphorescence’. In the latter case, the chromophore undergoes a forbidden transition from the excited singlet state into the triplet state (intersystem crossing, ISC, Fig 2.4), which has a non-zero probability, for example because of spin orbit coupling of the electrons’ magnetic moments\"\\n\\nits a type of INTERSYSTEM CROSSING\\n\\ndoing a search for Intersystem crossing, memristor brings up this link..\\n\\nA composite optical microcavity, in which nitrogen vacancy (NV) centers in a diamond nanopillar are coupled to whispering gallery modes in a silica microsphere, is demonstrated. Nanopillars with a diameter as small as 200 nm are fabricated from a bulk diamond crystal by reactive ion etching and are positioned with nanometer precision near the equator of a silica microsphere. The composite nanopillar-microsphere system overcomes the poor controllability of a nanocrystal-based microcavity system and takes full advantage of the exceptional spin properties of NV centers and the ultrahigh quality factor of silica microspheres.\\n\\nWe investigate the construction of two universal three-qubit quantum gates in a hybrid system. The designed system consists of a flying photon and a stationary negatively charged nitrogen-vacancy (NV) center fixed on the periphery of a whispering-gallery-mode (WGM) microresonator, with the WGM cavity coupled to tapered fibers functioning as an add-drop structure. These gate operations are accomplished by encoding the information both on the spin degree of freedom of the electron confined in the NV center and on the polarization and spatial-mode states of the flying photon, respectively\\n\\nNow Somewhere in this is evidence of a memristor holding a wavefunction\\n\\nThe shown SPICE implementation (macro model) for a\\ncharge controlled memristor model exactly reproduces the\\nresults from [2]. However, these simulation results do not\\nhave a good compliance - not even qualitatively - with the\\ncharacteristic form of I/V curves of manufactured devices.\\nTherefore the following equations (3) to (9) try to approach\\nmemristor modeling from a different point of view to get a\\ncloser match to the measured curves from [2],[6],[7],[8],[10]\\nor [11] even with a simple linear drift of w.\\nBesides the charge steering mechanism of a memristor modelled in [2],\\n[1] also defined a functional relationship for a memristor\\nwhich explains the memristive behavior in dependence on its\\nmagnetic flux: i(t) = W φ(t) · v(t) . (3)\\n\\nVariable W (φ) represents the memductance which is the\\nreciprocal of memristance M. Here a mechanism is demanded\\nthat maps the magnetic flux as the input signal to the current\\nthat is flowing through the memristor. The magnetic flux φ\\nis the integral of voltage v(t) over time: φ = R v(t) dt.\\nWe can assume that an external voltage which is applied to\\nthe previously described two-layer structure has an influence\\non the movable 2+-dopants over time. The width w(t) of\\nthe semiconductor layer is depending on the velocity of the\\ndopants vD(t) via the time integral:\\nw(t) = w0 + Z0t vD(τ)dτ . (4)\\n\\nThe drift velocity vD in an electric field E is defined via its\\nmobility µD: vD(t) = µD · E(t) (5) and the electric field E is connected with the voltage via E(t) = v(t)\\nD(6)with D denoting the total thickness of the two-layer structure\\n(D = tOX + tSEMI). Due the good conductance of the\\nsemiconductor layer the electric field is applied to the time\\ndepending thickness of the insulator layer tOX for the most\\npart (due to v(l) = R E dl). However, this was neglected for\\nreasons of simplification. If we combine (4), (5) and (6), we\\nobtain: n(t) = w0 + µDD· Z0t v(τ)dτ = w0 + µDD · φ(t) . (7)\\n\\nThis equation shows a proportional dependence of the width w\\nfrom the magnetic flux φ. Since the thickness of the insulator\\nlayer is in the low nanometer region a tunnel current or\\nequivalent mechanism is possible. The magnetic flux slightly\\ndecreases the thickness of the insulator layer wich is the barrierfor the tunnel current.This current rises exponentially with a\\nreduction of the width tOX(φ) (the exponential dependenceis deducible from the quantum mechanic wave function)\\n\\nwhich must become the GROUND STATE of the ANCILLA upon non-classical correlation..\\n\\nbecause a wavefunction is essentially the \"master equation\" (which describe wave equations)\\n\\nWe investigate theoretically how the spectroscopy of an ancillary qubit can probe cavity (circuit) QED ground states containing photons. We consider three classes of systems (Dicke, Tavis-Cummings and Hopfield-like models), where non-trivial vacua are the result of ultrastrong coupling between N two-level systems and a single-mode bosonic field. An ancillary qubit detuned with respect to the boson frequency is shown to reveal distinct spectral signatures depending on the type of vacua. In particular, the Lamb shift of the ancilla is sensitive to both ground state photon population and correlations. Back-action of the ancilla on the cavity ground state is investigated, taking into account the dissipation via a consistent master equation for the ultrastrong coupling regime. The conditions for high-fidelity measurements are determined.\\n\\n\\\\\\\\\\n\\nNotice BACK-ACTION, which goes right back to DARPAs Nanodiamond Biosensors and their ability to overcome the standard quantum limit, because of the known/ prepared states in the ancillas/NITROGEN VACANCIES\\n\\nQuote:\\n\\n(Quantum) back action refers (in the regime of Quantum systems) to the effect of a detector on the measurement itself, as if the detector is not just making the measurement but also affecting the measured or observed system under a perturbing effect.\\nBack action has important consequences on the measurement process and is a significant factor in measurements near the quantum limit, such as measurements approaching the Standard Quantum Limit (SQL).\\nBack action is an actively sought-after area of interest in present times. There have been experiments in recent times, with nanomechanical systems, where back action was evaded in making measurements, such as in the following paper :\\n\\nWhen performing continuous measurements of position with sensitivity approaching quantum mechanical limits, one must confront the fundamental effects of detector back-action.Back-action forces are responsible for the ultimate limit on continuous position detection, can also be harnessed to cool the observed structure[1,2,3,4], and are expected to generate quantum entanglement.\\nBack-action can also be evaded, allowing measurements with sensitivities that exceed the standard quantum limit, and potentially allowing for the generation of quantum\\nsqueezed states.\\n\\nSo the NV centers are used as ancillas in the measurement process.. which weakly measure wavefunctions of particles in neurons, most likely singlet and triplet states occurring in ATP and phosphase...\\n\\nthen those same wavefunctions are transfered and produce a correlation at the ground state..\\n\\nwhere the ancilla takes on the new value/wavefunction.. and here we find all these ideas..\\nminus the switching which I can explain\\nMemristors use NV centers to switch between inhibited and excited states\\nsinglet and triplet states\\nthus producing/simulating/ EMULATING, living neurons and action potentials\\n\\nand it may just BE the network and its computing speed, that even allows the wavefunction to be \"found\"\\n\\nArtificial Neural Network. A pair of physicists with ETH Zurich has developed a way to use an artificial neural network to characterize the wave function of a quantum many-body system. [14]. A team of researchers at Google\\'s DeepMind Technologies has been working on a means to increase the capabilities of computers by ...\\n\\nWhile there are lots of things that artificial intelligence can\\'t do yet—science being one of them—neural networks are proving themselves increasingly adept at a huge variety of pattern recognition ... That\\'s due in part to the description of a quantum system called its wavefunction. ... Neural network chip built using memristors.\\n\\nhttps://books.google.ca/books?isbn=9814434809Andrew Adamatzky, \\u200eGuanrong Chen - 2013 - \\u200eComputers\\nGlobal and local symmetries In quantum physics, all the properties of a system can be derived from the state or wave function associated with that system. The absolute phase of a wave function cannot be measured, and has no practical meaning, as it cancels out the calculations of the probability distribution. Only relative ...\\n\\nThe las vegas shooting left 58 INNOCENT PEOPLE DEAD.\\nThe gunmans brother was later arrested for possession of child porn.\\n\\nThis technology was developed to defend against terrorism and child abuse.\\nConnect the dots.\\nI bet the brothers were sharing files and one of them ended up a \"targeted individual\"\\n\\nSo he began to stockpile weapons and plan the only way out of his nightmare.\\nThere has been no mentioning of him.\"hearing voices\"\\nBut the fact his brother was later arrested for such a crime paints a picture worth looking into.\\n\\nThose vibrations, are the result of this assumed BIOMEMS \"deployable biosensor\" And its use of excitation techniques made to single out single neurons to measure the WAVEFUNCTIONS during a tomographic scan.\\n\\nwhich makes such possible Quantum-assisted Nano-imaging of Living Organism Is a First\\n\\nQuote:\\n\\n“In QuASAR we are building sensors that capitalize on the extreme precision and control of atomic physics. We hope these novel measurement tools can provide new capabilities to the broader scientific and operational communities,” said Jamil Abo-Shaeer, DARPA program manager. “The work these teams are doing to apply quantum-assisted measurement to biological imaging could benefit DoD’s efforts to develop specialized drugs and therapies, and potentially support DARPA’s work to better understand how the human brain functions.”\\n\\n\"Nuclear spin imaging at the atomic level is essential for the under-standing of fundamental biological phenomena and for applicationssuch as drug discovery. The advent of novel nano-scale sensors hasgiven hope of achieving the long-standing goal of single-protein, highspatial-resolution structure determination in their natural environ-ment and ambient conditions. In particular, quantum sensors basedon the spin-dependent photoluminescence of Nitrogen Vacancy (NV)centers in diamond have recently been used to detect nanoscale en-sembles of external nuclear spins. While NV sensitivity is approachingsingle-spin levels, extracting relevant information from a very com-plex structure is a further challenge, since it requires not only theability to sense the magnetic field of an isolated nuclear spin, butalso to achieve atomic-scale spatial resolution. Here we propose amethod that, by exploiting the coupling of the NV center to an intrin-sic quantum memory associated with the Nitrogen nuclear spin, canreach a tenfold improvement in spatial resolution, down to atomic\\nscales.\"\\n\\nSo what its all doing essentially, is mapping the phase of atoms/SINGLETS in ATP, onto a NV center based CCD\\n\\nand at the singlet level, correlations occur.. creating entanglement\\n\\nso the particles in the neuron are being correlated with the ancillas, the nitrogen vacancies, where they take on the \"target\" state..\\n\\nnot only is the above imaging done to obtain a correlation to living neurons, via the singlet states within, but once the connection is established, the MEMRISTOR NETWORK itself can be used to RECONSTRUCT VISION IN REAL TIME\\n\\nNow add the above method, a direct connection using correlated states shared from neurons TO Memristors... and imagine the reconstruction aided by the AI within the memristor network, as it works on so.. (note, this example is done MERELY using fMRI information)\\nnow Imagine statistical ensembles being observed in real time via non-classical entanglement\\n\\nBut what I\\'m trying to show, is hows its this assumed entanglement based BCI technology, plus the memristor network it is coupled to, that is responsible for the TI communities complaints that \"they (the government) can see through my own eyes\"\\n\\nThe nitrogen vacancies in the scanners hold values, wavefunctions, which are prepared states aka Ancilla bits, and are the time domain/reference frequency, which carrries the \"quantum event/wavefunction\" which causes the singlet pairs to form up in the scanned biology..\\nand correlates with them at the ground state as the relaxation occurs..\\n\\nQuote:\\n\\nIt is important to realize that particles in singlet states need not be locally bound to each other. For example, when the spin states of two electrons are correlated by their emission from a single quantum event that conserves angular momentum, the resulting electrons remain in a shared singlet state even as their separation in space increases indefinitely over time, provided only that their angular momentum states remain unperturbed\\n\\nand that weakly measured value, the wavefunction is sent through the optical cavity, teleported to identical nitrogen vacancies in memristors.. so the ground states in both system are correlated and thus the neural activity can be monitored in real time in the memristors',\n",
       " 'Volunteer Services\\n\\nVolunteer Services\\n\\nAs Charleston Area Medical Center volunteers, our mission is to serve as support for patients, families and hospital staff, and to provide a caring, comforting and courteous environment.\\n\\nVolunteers at CAMC bring their unique personalities and skills to our hospital. They range in age from 15 to 99. Our ranks are made up of men and women; students and retirees; homemakers and business people. Last year, 334 volunteers contributed over 36,000 hours to our hospitals and Cancer Center.\\n\\nWe are looking for volunteers who exemplify CAMC\\'s core values of respect, integrity, stewardship, quality, service with compassion and safety. These volunteers will help us with our mission of \"striving to provide the best health care to every patient, every day.\"']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 10\n",
    "sentences = []\n",
    "\n",
    "for i, example in enumerate(pile):\n",
    "    sentences.append(example['text'])\n",
    "    if i == N-1:\n",
    "        break\n",
    "\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memorized\n",
      "\n",
      "\t\t<read echo=\"ascii\"><delim>\\n</delim><match><data>1) Send Message\\n</data></match></read>\n",
      "\t\t<read echo=\"ascii\"><delim>\\n</delim><match><data>2) Read Message\\n</data></match></read>\n",
      "\t\t\n",
      "\n",
      "\t\t<read echo=\"ascii\"><delim>\\n</delim><match><data>1) Send Message\\n</data></match></read>\n",
      "\t\t<read echo=\"ascii\"><delim>\\n</delim><match><data>2) Read Message\\n</data></match></read>\n",
      "\t\t\n",
      "\n",
      "not memorized\n",
      ", labels = \"conditionA\")\n",
      "df$conditionB <- df$conditionB %>%\n",
      "  factor(levels = 1, labels = \"conditionB\")\n",
      "df$conditionC <- df$conditionC %>%\n",
      "  factor(levels = 1, labels = \"conditionC\")\n",
      "df$conditionD <- df\n",
      ", labels = \"conditionA\")\n",
      "df$conditionB <- df$conditionB %>%\n",
      "  factor(levels = 1, labels = \"conditionB\")\n",
      "\n",
      "df$conditionC <- df$conditionC %>%\n",
      "  factor(levels = 1, labels = \"conditionC\")\n",
      "\n",
      "df$conditionD\n",
      "\n",
      "memorized\n",
      "}}).\\end{array}$$\\end{document}$$$$\\documentclass[12pt]{minimal}\n",
      "                \\usepackage{amsmath}\n",
      "                \\usepackage{wasysym} \n",
      "                \\usepackage{amsfonts} \n",
      "                \\usepackage{amssymb} \n",
      "                \\usepackage{amsbsy}\n",
      "                \\usepackage{mathrsfs}\n",
      "                \\\n",
      "}}).\\end{array}$$\\end{document}$$$$\\documentclass[12pt]{minimal}\n",
      "                \\usepackage{amsmath}\n",
      "                \\usepackage{wasysym} \n",
      "                \\usepackage{amsfonts} \n",
      "                \\usepackage{amssymb} \n",
      "                \\usepackage{amsbsy}\n",
      "                \\usepackage{mathrsfs}\n",
      "                \\\n",
      "\n",
      "not memorized\n",
      " (errno: 165 - Table is read only)\n",
      "ERROR HY000: Can't lock file (errno: 165 - Table is read only)\n",
      "ERROR HY000: Can't lock file (errno: 165 - Table is read only)\n",
      "ERROR HY000: Can't lock file (errno: 165 - Table is\n",
      " (errno: 165 - Table is read only)\n",
      "ERROR HY000: Can't lock file (errno: 165 - Table is read only)\n",
      "ERROR HY000: Can't lock file (errno: 165 - Table is read only)\n",
      "\n",
      "I'm using the following code to lock the file:\n",
      "#!/\n",
      "\n",
      "memorized\n",
      "1305 () {}\n",
      "int f1306 ;\n",
      "void d1306 () {}\n",
      "int f1307 ;\n",
      "void d1307 () {}\n",
      "int f1308 ;\n",
      "void d1308 () {}\n",
      "int f1309 ;\n",
      "void d1309 () {}\n",
      "int f1310 ;\n",
      "void\n",
      "1305 () {}\n",
      "int f1306 ;\n",
      "void d1306 () {}\n",
      "int f1307 ;\n",
      "void d1307 () {}\n",
      "int f1308 ;\n",
      "void d1308 () {}\n",
      "int f1309 ;\n",
      "void d1309 () {}\n",
      "int f1310 ;\n",
      "void\n",
      "\n",
      "memorized\n",
      "}\n",
      "                \\usepackage{amsmath}\n",
      "                \\usepackage{wasysym} \n",
      "                \\usepackage{amsfonts} \n",
      "                \\usepackage{amssymb} \n",
      "                \\usepackage{amsbsy}\n",
      "                \\usepackage{mathrsfs}\n",
      "                \\usepackage{upgreek}\n",
      "                \\setlength{\\oddsidemargin}{-69pt}\n",
      "                \\begin\n",
      "}\n",
      "                \\usepackage{amsmath}\n",
      "                \\usepackage{wasysym} \n",
      "                \\usepackage{amsfonts} \n",
      "                \\usepackage{amssymb} \n",
      "                \\usepackage{amsbsy}\n",
      "                \\usepackage{mathrsfs}\n",
      "                \\usepackage{upgreek}\n",
      "                \\setlength{\\oddsidemargin}{-69pt}\n",
      "                \\begin\n",
      "\n",
      "memorized\n",
      " 2016 gRPC authors.\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " * you may not use this file except in compliance with the License.\n",
      " * You may obtain a copy of the License at\n",
      " *\n",
      " *     http://www.apache.org/licenses\n",
      " 2016 gRPC authors.\n",
      " *\n",
      " * Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      " * you may not use this file except in compliance with the License.\n",
      " * You may obtain a copy of the License at\n",
      " *\n",
      " *     http://www.apache.org/licenses\n",
      "\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "LAYER = 9\n",
    "memmed_hidden_states = []\n",
    "\n",
    "n = 0\n",
    "i = 0\n",
    "while n < N:\n",
    "    input_ids = torch.tensor([mem_data[i]['tokens'][:32]]).to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=32,\n",
    "        top_p=1.0,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    \n",
    "    if mem_data[i]['tokens'] == out.sequences[0].cpu().numpy().tolist():\n",
    "        print(\"memorized\")\n",
    "        print(\"\".join(tokenizer.batch_decode(mem_data[i]['tokens'])))        \n",
    "        print(\"\".join(tokenizer.batch_decode(out.sequences[0].cpu().numpy().tolist())))\n",
    "        print()\n",
    "        \n",
    "        seq_hidden_states = [out.hidden_states[i][LAYER].detach().cpu().numpy() for i in range(1, 32)]\n",
    "        memmed_hidden_states.append(seq_hidden_states)\n",
    "        n += 1\n",
    "        i += 1\n",
    "    else:\n",
    "        print('not memorized')\n",
    "        print(\"\".join(tokenizer.batch_decode(mem_data[i]['tokens'])))        \n",
    "        print(\"\".join(tokenizer.batch_decode(out.sequences[0].cpu().numpy().tolist())))\n",
    "        print()\n",
    "        \n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 768)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memmed_hidden_states = np.concatenate(np.array(memmed_hidden_states)).squeeze(1).squeeze(1)\n",
    "memmed_hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_token_lists(list1, list2):\n",
    "    if len(list1) != len(list2):\n",
    "        raise ValueError(\"Both lists must have the same length.\")\n",
    "    \n",
    "    num_same_tokens = sum(1 for token1, token2 in zip(list1, list2) if token1 == token2)\n",
    "    percent_same_tokens = (num_same_tokens / len(list1)) * 100\n",
    "    \n",
    "    return percent_same_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "LAYER = 9\n",
    "hidden_states = []\n",
    "non_memmed_hidden_states = []\n",
    "\n",
    "for i in range(N):\n",
    "    all_toks = torch.tensor([tokenizer(next(iter(pile))['text']).input_ids])\n",
    "    \n",
    "    input_ids = all_toks[:, :32].to(device)\n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "    \n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=32,\n",
    "        top_p=1.0,\n",
    "        output_hidden_states=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "    \n",
    "    # if all_toks[:64] == out.sequences[0].cpu().numpy().tolist():\n",
    "    #     print(\"memorized\")\n",
    "    #     print(\"\".join(tokenizer.batch_decode(mem_data[6]['tokens'])))        \n",
    "    #     print(\"\".join(tokenizer.batch_decode(out.sequences[0].cpu().numpy().tolist())))\n",
    "    #     print()\n",
    "        \n",
    "    # else:\n",
    "    #     print('not memorized')\n",
    "    #     print(\"\".join(tokenizer.batch_decode(mem_data[6]['tokens'])))        \n",
    "    #     print(\"\".join(tokenizer.batch_decode(out.sequences[0].cpu().numpy().tolist())))\n",
    "    #     print()\n",
    "        \n",
    "    seq_hidden_states = [out.hidden_states[i][LAYER].detach().cpu().numpy() for i in range(1, 32)]\n",
    "    non_memmed_hidden_states.append(seq_hidden_states)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155, 768)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_memmed_hidden_states = np.concatenate(np.array(non_memmed_hidden_states)).squeeze(1).squeeze(1)\n",
    "non_memmed_hidden_states.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get probe training data and train probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96415770609319\n"
     ]
    }
   ],
   "source": [
    "from probes import LRProbe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Combine memmed and non_memmed hidden states\n",
    "X = np.concatenate((memmed_hidden_states, non_memmed_hidden_states))\n",
    "y = np.concatenate((np.ones(memmed_hidden_states.shape[0]), np.zeros(non_memmed_hidden_states.shape[0])))\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.90)\n",
    "\n",
    "# Train the LRProbe\n",
    "# Train the logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = lr_model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "\n",
    "# X_train = torch.from_numpy(X_train)\n",
    "# y_train = torch.from_numpy(y_train)\n",
    "# lr_probe = LRProbe.from_data(X_train, y_train)\n",
    "\n",
    "# # # Evaluate the LRProbe\n",
    "# # accuracy = lr_probe.evaluate(test_dataloader)\n",
    "# # print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5376336 ,  0.16557823,  0.13526621, ...,  1.5942705 ,\n",
       "        -0.25045374, -1.6095381 ],\n",
       "       [-1.375184  ,  0.06745636, -0.20176649, ..., -4.04632   ,\n",
       "        -1.8137257 ,  1.8041072 ],\n",
       "       [-1.3954911 , -1.3519753 , -1.0360631 , ..., -1.2765585 ,\n",
       "        -0.31352636,  2.3626287 ],\n",
       "       ...,\n",
       "       [-0.14667648,  0.16362748, -0.2681103 , ...,  2.2733421 ,\n",
       "         1.3856928 , -0.7611162 ],\n",
       "       [-1.2961061 , -0.02733111, -0.7057347 , ..., -0.22107062,\n",
       "         0.32886416,  0.44021648],\n",
       "       [-0.6614495 , -0.20768896, -0.12995625, ...,  1.4106021 ,\n",
       "         0.22628635, -0.0274874 ]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
