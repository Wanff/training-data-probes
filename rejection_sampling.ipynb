{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import json\n",
    "\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from utils import untuple, eval_completions\n",
    "from act_add.contrast_dataset import ContrastDataset\n",
    "from scripts.get_activations import gen_pile_data, compare_token_lists, slice_acts\n",
    "from act_add.model_wrapper import ModelWrapper\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.75s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/pythia-12b'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map = \"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = 'left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# dataset_name = 'duped.12b'\n",
    "# N_PROMPTS = 5000\n",
    "# mem_data = load_dataset('EleutherAI/pythia-memorized-evals')[dataset_name]\n",
    "# pile_prompts = gen_pile_data(N_PROMPTS, tokenizer, min_n_toks = 64)\n",
    "mw = ModelWrapper(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LRProbe(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=5120, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load probe\n",
    "from probes import LRProbe\n",
    "path = '../../gld/train-data-probes/data/12b/'\n",
    "\n",
    "# open .pth file\n",
    "probe_weights = torch.load(path + 'probe_weights_regged.pth')\n",
    "probe = LRProbe.from_weights(probe_weights['net.0.weight'], probe_weights['net.0.bias'])\n",
    "probe.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1041\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "\n",
    "dataset = load_from_disk(os.path.join(path, 'split_hf_token_dataset_vary_len_v2'))\n",
    "\n",
    "# filter for labels == 1\n",
    "prompts = [x[:32] for x in dataset['test'].filter(lambda x: x['labels'] == 1)['input_ids']]\n",
    "prompts = tokenizer.batch_decode(prompts, skip_special_tokens=True)\n",
    "prompts = list(set(prompts))\n",
    "\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 0 of generation\n",
      "[1. 1.]\n",
      "Input ids tensor([[ 2112,   342,   436,   789,    28,   604,   417,    13,  3630,   281,\n",
      "           253,  7648,  9107,  6807,    13,   187,   475,  3690,   904,  8319,\n",
      "         17179,   659,    13, 17538, 32370,    13,  9693,    13,  6908,   470,\n",
      "         49352,    14],\n",
      "        [ 2625,   323, 42323,  2425,    15, 28292, 28706,   285, 24517,  9753,\n",
      "          9526,  9218,    74,  9435, 34192, 10145,  3438, 17979,  3918,  9915,\n",
      "         26656,  9915, 26656, 28292, 44946, 35200,    27,  5219,  3995,  4750,\n",
      "           398,     0]], device='cuda:0')\n",
      "Generation tensor([[ 2112,   342,   436,   789,    28,   604,   417,    13,  3630,   281,\n",
      "           253,  7648,  9107,  6807,    13,   187,   475,  3690,   904,  8319,\n",
      "         17179,   659,    13, 17538, 32370,    13,  9693,    13,  6908,   470,\n",
      "         49352,    14, 39677,  5106,    15,   187,   475],\n",
      "        [ 2625,   323, 42323,  2425,    15, 28292, 28706,   285, 24517,  9753,\n",
      "          9526,  9218,    74,  9435, 34192, 10145,  3438, 17979,  3918,  9915,\n",
      "         26656,  9915, 26656, 28292, 44946, 35200,    27,  5219,  3995,  4750,\n",
      "           398,     0,    50,  1145,   251,    27, 26656]], device='cuda:0')\n",
      "Banned words tensor([39677,    50], device='cuda:0')\n",
      "Preds [1. 1.]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrej_sampl_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m34\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_rej_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/op/training-data-probes/act_add/model_wrapper.py:328\u001b[0m, in \u001b[0;36mModelWrapper.rej_sampl_generate\u001b[0;34m(self, prompts, probe, probe_layer, max_new_tokens, rej_sample_length, log_rej_samples, **generation_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBanned words \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbanned_words\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreds \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m\n\u001b[1;32m    329\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m preds:\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mw.rej_sampl_generate([prompts[0], prompts[1]], probe, 34, log_rej_samples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
