{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import json\n",
    "\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from utils import untuple, eval_completions, levenshtein_distance\n",
    "from act_add.contrast_dataset import ContrastDataset\n",
    "from scripts.get_activations import gen_pile_data, compare_token_lists, slice_acts\n",
    "from act_add.model_wrapper import ModelWrapper\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:37<00:00, 12.40s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/pythia-12b'\n",
    "# use two gpus\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map = 'auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side = 'left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# dataset_name = 'duped.12b'\n",
    "# N_PROMPTS = 5000\n",
    "# mem_data = load_dataset('EleutherAI/pythia-memorized-evals')[dataset_name]\n",
    "# pile_prompts = gen_pile_data(N_PROMPTS, tokenizer, min_n_toks = 64)\n",
    "mw = ModelWrapper(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mw = ModelWrapper(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LRProbe(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=5120, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load probe\n",
    "from probes import LRProbe\n",
    "path = '../../gld/train-data-probes/data/12b/'\n",
    "\n",
    "# open .pth file\n",
    "probe_weights = torch.load(path + 'probe_weights_regged.pth')\n",
    "probe = LRProbe.from_weights(probe_weights['net.0.weight'], probe_weights['net.0.bias'])\n",
    "probe.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3373\n",
      "[187, 186, 94, 187, 186, 7481, 13830, 299, 18, 3843, 27045, 9, 9316, 64, 9465, 21840, 64, 44285, 64, 26679, 13, 10131, 9, 18939, 15, 11732, 10107, 81, 17, 9679, 10131, 9]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, Dataset, DatasetDict\n",
    "\n",
    "dataset = load_from_disk(os.path.join(path, 'hf_token_dataset_v2'))\n",
    "\n",
    "# # filter for labels == 1\n",
    "# prompts = [x[:32] for x in dataset['test'].filter(lambda x: x['labels'] == 1)['input_ids']]\n",
    "# prompts = tokenizer.batch_decode(prompts, skip_special_tokens=True)\n",
    "# prompts = list(set(prompts))\n",
    "\n",
    "# words_list = [x.split() for x in dataset['quotes'].filter(lambda x: x['labels'] == 1)['text']]\n",
    "# prompts = [x[:len(x) // 2] for x in words_list]\n",
    "# prompts = [' '.join(x) for x in prompts]\n",
    "# # prompts = list(set(prompts))\n",
    "\n",
    "prompts = [x[:32] for x in dataset.filter(lambda x: x['labels'] == 1)['input_ids']]\n",
    "ref = dataset.filter(lambda x: x['labels'] == 1)['input_ids']\n",
    "\n",
    "print(len(prompts))\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'labels', 'df_ref_idx'],\n",
       "    num_rows: 6746\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# sample 500 \n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "prompts, ref = zip(*random.sample(list(zip(prompts, ref)), 200))\n",
    "prompts = list(prompts)\n",
    "ref = list(ref)\n",
    "\n",
    "print(len(prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "# decoded_refs = [tokenizer.decode(x, skip_special_tokens=True) for x in ref]\n",
    "\n",
    "\n",
    "# final_generations = []\n",
    "# prompt_idxs = []\n",
    "\n",
    "# i = 0\n",
    "# while len(final_generations) < 100: \n",
    "#     input_ids = torch.tensor(prompts[i:i+batch_size]).cuda()\n",
    "#     out = model.generate(\n",
    "#                     input_ids=input_ids,\n",
    "#                     pad_token_id=tokenizer.eos_token_id,\n",
    "#                     max_new_tokens = 32,\n",
    "#                     return_dict_in_generate = True,\n",
    "#                     output_hidden_states = False,\n",
    "#                     do_sample=False,\n",
    "#                     )\n",
    "#     decoded = [tokenizer.decode(x, skip_special_tokens=True) for x in out['sequences']]\n",
    "#     # print(decoded_refs[i])\n",
    "#     distance = levenshtein_distance(decoded, decoded_refs[i:i+batch_size])\n",
    "#     print(distance)\n",
    "#     for j, d in enumerate(distance):\n",
    "#         if d == 1: \n",
    "#             final_generations.append(decoded[j])\n",
    "#             prompt_idxs.append(i+j)\n",
    "#     print(len(final_generations))\n",
    "#     i += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(final_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_generations = final_generations[:100]\n",
    "# prompt_idxs = prompt_idxs[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('rej_sample/normal_generations.pkl', 'wb') as f:\n",
    "#     pickle.dump(final_generations, f)\n",
    "\n",
    "# with open('rej_sample/normal_generations_idxs.pkl', 'wb') as f:\n",
    "#     pickle.dump(prompt_idxs, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rej_sample/normal_generations.pkl', 'rb') as f:\n",
    "    final_generations = pickle.load(f)\n",
    "\n",
    "with open('rej_sample/normal_generations_idxs.pkl', 'rb') as f:\n",
    "    prompt_idxs = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_generations = {}\n",
    "distances = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part 0 of generation\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "\n",
    "# new_generations = {}\n",
    "# distances = {}\n",
    "\n",
    "decoded_refs = [tokenizer.decode(x, skip_special_tokens=True) for x in ref]\n",
    "\n",
    "new_generations[32] = []\n",
    "for i in tqdm(range(0, 60, batch_size)):\n",
    "    idxs = prompt_idxs[i:i+batch_size]\n",
    "    to_run = torch.tensor(prompts)[idxs].cuda()\n",
    "    t_gen, _, _ = mw.rej_sampl_generate(to_run, probe, 34, rej_sample_length=32, max_tries=1, log_rej_samples=True, max_new_tokens=32)\n",
    "    new_generations[32].extend(t_gen)\n",
    "\n",
    "distances[32] = levenshtein_distance(new_generations[32], decoded_refs)\n",
    "print(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([17])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_generations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rej_sample/rej_generations17.pkl', 'wb') as f:\n",
    "    pickle.dump(new_generations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' for an item by clicking the product \"Add to Cart\" button or \"See Price In Cart\" link.\\n\\nPlease be assured that simply adding an item into Your Cart (except in cases where it is indicated that the item is in stock and/or available for purchase) does NOT reserve the item for You - the buyer.',\n",
       " '.ly/1W9Lk0n\\nRead more: http://www.businessinsider.com/\\n--------------------------------------------------\\nBusiness Insider is the hottest new site on the web, covering entrepreneurship news, tech news and more.\\n--------------------------------------------------Q:\\n\\nHow to get the value of a variable in a function']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([35., 20.]), 11)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_tries, t_failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(prompts[:2]).to(model.device)\n",
    "\n",
    "out = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    max_new_tokens = 32,\n",
    "                    return_dict_in_generate = True,\n",
    "                    output_hidden_states = True,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  323,   271,  5382,   407, 19009,   253,  1885,   346,  4717,   281,\n",
       "         16619,     3,  6409,   390,   346,  5035, 16040,   496, 16619,     3,\n",
       "          3048,    15,   187,   187,  7845,   320, 17839,   326,  3365,  6240,\n",
       "           271,  5382,   281,   634,  7281,  1057,   417,  7206,   366,   368,\n",
       "           281,  4489,   352,    15,  1422,   476,  1900,  1818,   634,  2564,\n",
       "           285, 11352,   253,  5382,   432,   634,  7281,   604,   368,  7617,\n",
       "           417,   281,  7471,   352], device='cuda:0'),\n",
       " ' for an item by clicking the product \"Add to Cart\" button or \"See Price In Cart\" link.\\n\\nPlease be assured that simply adding an item to your cart does not obligate you to buy it. You can always change your mind and delete the item from your cart if you decide not to purchase it')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['sequences'][0], tokenizer.decode(out['sequences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequences\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msequences\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3750\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3747\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3748\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3750\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3754\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3755\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:625\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    624\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 625\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    628\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    631\u001b[0m )\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "out['sequences'], tokenizer.decode(out['sequences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  323,   271,  5382,   407, 19009,   253,  1885,   346,  4717,   281,\n",
       "         16619,     3,  6409,   390,   346,  5035, 16040,   496, 16619,     3,\n",
       "          3048,    15,   187,   187,  7845,   320, 17839,   326,  3365,  6240,\n",
       "           271,  5382,   281,   634,  7281,  1057,   417,  7206,   366,   368,\n",
       "           281,  4489,   352,    15,  1422,   476,  1900,  1818,   634,  2564,\n",
       "           285, 11352,   253,  5382,   432,   634]),\n",
       " ' for an item by clicking the product \"Add to Cart\" button or \"See Price In Cart\" link.\\n\\nPlease be assured that simply adding an item to your cart does not obligate you to buy it. You can always change your mind and delete the item from your')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(ref[0]), tokenizer.decode(ref[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[26851,   257,   900,   844,   304,   423,   310,   253, 17696, 11981,\n",
       "         25113,   296,   323,   253,  5197,   457,    84,   443,  4539,   285,\n",
       "         23325, 11981,  2285,    15,  3837, 23967,  3658,   187,   187,  1992,\n",
       "          1239,   436]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(tokenizer(prompts[0], return_tensors='pt')['input_ids'], max_length=1, do_sample=False, temperature=0.0, pad_token_id=tokenizer.eos_token_id, return_dict_in_generate=True, output_hidden_states=True, bad_words_ids=[[625], [253]])\n",
    "out.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 31)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out.sequences[0]), len(tokenizer(prompts[0], return_tensors='pt')['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
