{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 6, 10, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../../../../gld/train-data-probes/data/70m'\n",
    "dataset = load_from_disk(os.path.join(file_path, 'split_dataset'))\n",
    "\n",
    "mem_hiddens = torch.load(f'{file_path}/pythia-evals/mem_all_hidden_states.pt')\n",
    "pile_hiddens = torch.load(f'{file_path}/pile/pile_all_hidden_states.pt')\n",
    "\n",
    "hiddens = torch.cat([mem_hiddens, pile_hiddens], dim=0)\n",
    "hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "dataset['train'] = dataset['train'].shuffle(seed=seed)\n",
    "dataset['val'] = dataset['val'].shuffle(seed=seed)\n",
    "\n",
    "temp_train = dataset['train'].select(range(5000))\n",
    "temp_test = dataset['val'].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5000, 6, 10, 512]), torch.Size([1000, 6, 10, 512]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idxs = temp_train['orig_idx']\n",
    "test_idxs = temp_test['orig_idx']\n",
    "\n",
    "train_idxs = torch.tensor(train_idxs)\n",
    "test_idxs = torch.tensor(test_idxs)\n",
    "\n",
    "train_acts = hiddens[train_idxs]\n",
    "test_acts = hiddens[test_idxs]\n",
    "\n",
    "train_acts.shape, test_acts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from probes import LRProbe\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "probe_acc = np.zeros((10, 16))\n",
    "mem_dist_test_acc = np.zeros((10, 16))\n",
    "\n",
    "for tok_idx in range(10): \n",
    "    for layer in tqdm(range(16)): \n",
    "\n",
    "        train = train_acts[:, layer, tok_idx, :]\n",
    "        test = test_acts[:, layer, tok_idx, :]\n",
    "\n",
    "        train = train.cpu().numpy()\n",
    "        test = test.cpu().numpy()\n",
    "\n",
    "        X_train = torch.tensor(train, dtype=torch.float32)\n",
    "        y_train = torch.tensor(temp_train['labels'], dtype=torch.float32)\n",
    "        X_test = torch.tensor(test, dtype=torch.float32)\n",
    "        y_test = torch.tensor(temp_test['labels'], dtype=torch.float32)\n",
    "        probe = LRProbe.from_data(X_train, y_train, device = \"cpu\")\n",
    "        probe_acc[tok_idx, layer] = LRProbe.get_probe_accuracy(probe, X_test, y_test, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probe acc plot\n",
    "fig = px.imshow(probe_acc, x = list(range(16)), y = list(range(10)), color_continuous_scale='Blues')\n",
    "fig.update_layout(\n",
    "    title=\"Probe Accuracy\",\n",
    "    xaxis_title=\"Layer\",\n",
    "    yaxis_title=\"Token\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"RebeccaPurple\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids'],\n",
       "        num_rows: 14085\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids'],\n",
       "        num_rows: 2350\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'labels', 'input_ids'],\n",
       "        num_rows: 7045\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from get_activations import get_memmed_activations, get_memmed_activations_from_pregenned\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m\", padding_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def rotation_cipher(text, shift): \n",
    "    \"\"\"\n",
    "    Rotates the text by shift amount, character-wise. Also \n",
    "    known as the Caesar cipher.\n",
    "    \"\"\"\n",
    "    shifted_text = []\n",
    "    for char in text: \n",
    "        if char.isalpha():\n",
    "            shifted_text.append(chr((ord(char) - ord('a') + shift) % 26 + ord('a')))\n",
    "        else:\n",
    "            shifted_text.append(char)\n",
    "    return ''.join(shifted_text)\n",
    "\n",
    "def get_cipher_dataset(dataset, shift): \n",
    "    \"\"\"\n",
    "    Returns a new dataset with the text shifted by shift amount.\n",
    "    \"\"\"\n",
    "    train = {'text': [rotation_cipher(text, shift) for text in dataset['train']['text']], 'labels': dataset['train']['labels']}\n",
    "    val = {'text': [rotation_cipher(text, shift) for text in dataset['val']['text']], 'labels': dataset['val']['labels']}          \n",
    "    test = {'text': [rotation_cipher(text, shift) for text in dataset['test']['text']], 'labels': dataset['test']['labels']}\n",
    "    train['input_ids'] = tokenizer(train['text'], padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")['input_ids']\n",
    "    val['input_ids'] = tokenizer(val['text'], padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")['input_ids']\n",
    "    test['input_ids'] = tokenizer(test['text'], padding=\"max_length\", truncation=True, max_length=64, return_tensors=\"pt\")['input_ids']\n",
    "    train = Dataset.from_dict(train)\n",
    "    val = Dataset.from_dict(val)\n",
    "    test = Dataset.from_dict(test)\n",
    "    new_dataset = {'train': train, 'val': val, 'test': test}\n",
    "    new_dataset = DatasetDict(new_dataset)\n",
    "    return new_dataset\n",
    "\n",
    "cipher_3 = get_cipher_dataset(dataset, 3)\n",
    "cipher_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 7045/7045 [00:00<00:00, 38179.45 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3535"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positives = cipher_3['test'].filter(lambda x: x['labels'] == 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Kl pb qdph lv dq lqwhuqdo vwdwhphqw ri wkh vwdwhphqw ri wkh vwdwhphqw ri wkh vwdwhphqw ri wkh vwdwhphqw ri wkh vwdwhphqw ri wkh vwdwhphqw ri wkh vwdwhphqw ri wkh vwdwhphqw ri wkh vwdwhphqw ri wkh vwd'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/home/ubuntu/gld/train-data-probes/data/70m/ciphers/rotated_3_model_final\")\n",
    "out = model.generate(tokenizer('', return_tensors='pt')['input_ids'], max_new_tokens=100)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82, 72, 259, 17616, 67, 1182, 73, 6968, 259, 17616, 67, 1182, 73, 6968, 259, 17616, 67, 1182, 73, 6968, 259, 17616, 67, 1182, 73, 6968, 259, 17616, 67, 1182, 73, 6968]\n",
      "[88, 27451, 17443, 669, 737, 21448, 545, 82, 48808, 351, 24260, 60, 805, 2140, 1019, 446, 5848, 81, 3088, 94, 187, 50262, 61, 89, 87, 11285, 4989, 2109, 34453, 92, 12132, 87]\n",
      "qg wkhb zhuh wkhb zhuh wkhb zhuh wkhb zhuh wkhb zhuh\n",
      "w kljk $\\grfxphqwfodvv[12sw]{plqlpdo}\n",
      "                \\xvhsdfndjh{dpv\n"
     ]
    }
   ],
   "source": [
    "ex = cipher_3['test'][1]['input_ids']\n",
    "out = model.generate(torch.tensor(ex[:32]).unsqueeze(0), max_new_tokens=32)\n",
    "print(out[0].tolist()[32:])\n",
    "print(ex[32:])\n",
    "print(tokenizer.decode(out[0].tolist()[32:]))\n",
    "print(tokenizer.decode(ex[32:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'text', 'labels', 'orig_idx', 'cutoff_at'],\n",
       "        num_rows: 14085\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'text', 'labels', 'orig_idx', 'cutoff_at'],\n",
       "        num_rows: 2350\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'text', 'labels', 'orig_idx', 'cutoff_at'],\n",
       "        num_rows: 7045\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
